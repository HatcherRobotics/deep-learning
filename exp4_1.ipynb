{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "# 定义dataset\n",
    "class my_Dataset(data.Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.X = features\n",
    "        self.y = labels\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_data = pd.read_csv('./foursquare-checkin/FS_NYC.csv')\n",
    "venue_id2index={id:index for index, id in enumerate(nyc_data['venueId'].drop_duplicates())}\n",
    "nyc_data['venueIndex'] = nyc_data['venueId'].map(venue_id2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38332\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(nyc_data['venueIndex'].max())\n",
    "print(nyc_data['venueIndex'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FourSquareDataset:\n",
    "    def __init__(self, data_path):\n",
    "        # 适当修改数据读取过程\n",
    "        self.nyc_data = pd.read_csv(data_path)\n",
    "        self.venue_id2index={id:index for index, id in enumerate(self.nyc_data['venueId'].drop_duplicates())}\n",
    "        self.nyc_data['venueIndex'] = self.nyc_data['venueId'].map(self.venue_id2index)\n",
    "        self.min = self.nyc_data['venueIndex'].min()\n",
    "        self.max = self.nyc_data['venueIndex'].max()\n",
    "        self.nyc_data['venueIndex'] = (self.nyc_data['venueIndex'] - self.min) / (self.max - self.min)\n",
    "\n",
    "    def denormalize(self, x):\n",
    "        return x * (self.max - self.min) + self.min\n",
    "\n",
    "    def construct_set(self, train_por, val_por, test_por, window_size, label=0):\n",
    "        train_x = []\n",
    "        train_y = []\n",
    "        val_x = []\n",
    "        val_y = []\n",
    "        test_x = []\n",
    "        test_y = []\n",
    "\n",
    "        # 补全构造过程\n",
    "        for user_id, group in self.nyc_data.groupby('userId'):\n",
    "            # pandas会对userId进行遍历。\n",
    "            # 每次遍历中，group包含了对应userId所有的check-in记录。\n",
    "            user_trajectory = group.sort_values(['utcTimestamp'])['venueIndex'].tolist()\n",
    "\n",
    "            user_trajectory_length = int(len(user_trajectory))\n",
    "            train_trajectory_length = int(user_trajectory_length*train_por)\n",
    "            val_trajectory_length = int(user_trajectory_length*val_por)\n",
    "\n",
    "            train_trajectory = user_trajectory[:train_trajectory_length]\n",
    "            val_trajectory = user_trajectory[train_trajectory_length:val_trajectory_length+train_trajectory_length]\n",
    "            test_trajectory = user_trajectory[train_trajectory_length+val_trajectory_length:]\n",
    "\n",
    "            for i in range(len(train_trajectory) - window_size):\n",
    "                train_x.append(train_trajectory[i:i+window_size])\n",
    "                train_y.append(train_trajectory[i+window_size])\n",
    "            \n",
    "            for i in range(len(val_trajectory) - window_size):\n",
    "                val_x.append(val_trajectory[i:i+window_size])\n",
    "                val_y.append(val_trajectory[i+window_size])\n",
    "\n",
    "            for i in range(len(test_trajectory) - window_size):\n",
    "                test_x.append(test_trajectory[i:i+window_size])\n",
    "                test_y.append(test_trajectory[i+window_size])            \n",
    "           \n",
    "        train_x = torch.tensor(train_x).unsqueeze(2)\n",
    "        train_y = torch.tensor(train_y).type(torch.LongTensor)\n",
    "        val_x = torch.tensor(val_x).unsqueeze(2)\n",
    "        val_y = torch.tensor(val_y).type(torch.LongTensor)\n",
    "        test_x = torch.tensor(test_x).unsqueeze(2)\n",
    "        test_y = torch.tensor(test_y).type(torch.LongTensor)\n",
    "        print(train_x.shape)\n",
    "        print(train_y.shape)\n",
    "        print(train_y.type())\n",
    "        train_set = my_Dataset(train_x, train_y)\n",
    "        val_set = my_Dataset(val_x, val_y)\n",
    "        test_set = my_Dataset(test_x, test_y)\n",
    "        return train_set, val_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([123027, 12, 1])\n",
      "torch.Size([123027])\n",
      "torch.LongTensor\n"
     ]
    }
   ],
   "source": [
    "FourSquareData = FourSquareDataset('./foursquare-checkin/FS_NYC.csv')\n",
    "train_set, val_set, test_set = FourSquareData.construct_set(train_por=0.6,val_por=0.2,test_por=0.2,window_size=12)\n",
    "batch_size = 64\n",
    "train_loader = data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=2, drop_last=True)\n",
    "val_loader = data.DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=2, drop_last=True)\n",
    "test_loader = data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=2, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        \"\"\"\n",
    "        :param input_size: 指定输入数据的维度。例如，对于简单的时间序列预测问题，每一步的输入均为一个采样值，因此input_size=1.\n",
    "        :param hidden_size: 指定隐藏状态的维度。这个值并不受输入和输出控制，但会影响模型的容量。\n",
    "        :param output_size: 指定输出数据的维度。此值取决于具体的预测要求。例如，对简单的时间序列预测问题，output_size=1.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # 可学习参数的维度设置，可以类比一下全连接网络的实现。其维度取决于输入数据的维度，以及指定的隐藏状态维度。\n",
    "        self.w_h = nn.Parameter(torch.rand(input_size, hidden_size))\n",
    "        self.u_h = nn.Parameter(torch.rand(hidden_size, hidden_size))\n",
    "        self.b_h = nn.Parameter(torch.zeros(hidden_size))\n",
    "\n",
    "        self.w_y = nn.Parameter(torch.rand(hidden_size, output_size))\n",
    "        self.b_y = nn.Parameter(torch.zeros(output_size))\n",
    "\n",
    "        # 准备激活函数。Dropout函数可选。\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.leaky_relu = nn.LeakyReLU()\n",
    "\n",
    "        # 可选：使用性能更好的参数初始化函数\n",
    "        for param in self.parameters():\n",
    "            if param.dim() > 1:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: 输入序列。一般来说，此输入包含三个维度：batch，序列长度，以及每条数据的特征。\n",
    "        \"\"\"\n",
    "        #print(x.shape)\n",
    "        batch_size = x.size(0)\n",
    "        seq_len = x.size(1)\n",
    "        # x batch_size X seq_len X input_size\n",
    "        # 初始化隐藏状态，一般设为全0。由于是内部新建的变量，需要同步设备位置。\n",
    "        h = torch.zeros(batch_size, self.hidden_size).to(x.device)\n",
    "        # RNN实际上只能一步一步处理序列。因此需要用循环迭代。\n",
    "        y_list = []\n",
    "        for i in range(seq_len):\n",
    "            h = self.tanh(torch.matmul(\n",
    "                x[:, i, :], self.w_h) + torch.matmul(h, self.u_h) + self.b_h)\n",
    "            # (batch_size, output_size)\n",
    "            y = self.leaky_relu(torch.matmul(h, self.w_y) + self.b_y)\n",
    "            y_list.append(y)  # seq_len X batch_size X output_size\n",
    "        # 一般来说，RNN的返回值为最后一步的隐藏状态，以及每一步的输出状态。\n",
    "        out_y = torch.stack(y_list, dim=1)\n",
    "        y_hat = out_y[:, -1, :]\n",
    "        #print(out_y.shape)\n",
    "        #print(y_hat.shape)\n",
    "        #print(out_y.shape)\n",
    "        #print(h.shape)\n",
    "        return out_y, h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 64 38333\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_size = train_set.X.shape[-1]\n",
    "hidden_size = 64\n",
    "\n",
    "output_size = 38333\n",
    "\n",
    "print(input_size, hidden_size, output_size)\n",
    "seq_len = 12\n",
    "lr = 0.0001\n",
    "epochs = 80\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "my_rnn = MyRNN(input_size, hidden_size, output_size).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(my_rnn.parameters(), lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iter,net,loss):\n",
    "    acc_sum,n=0.0,0\n",
    "    test_l_sum=0.0\n",
    "    for X,y in data_iter:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)        \n",
    "        acc_sum+=(net(X).argmax(dim=1)==y).float().sum().item()\n",
    "        l=loss(net(X),y).sum()\n",
    "        test_l_sum+=l.item()\n",
    "        n+=y.shape[0]\n",
    "    return acc_sum/n,test_l_sum/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net,train_iter,test_iter,loss,num_epochs,batch_size,params=None,lr=None,optimizer=None):\n",
    "    train_loss=[]\n",
    "    test_loss=[]\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum,train_acc_sum,n=0.0,0.0,0\n",
    "        for X,y in train_iter:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            y_hat,h = net(X)\n",
    "            y_hat=y_hat[:,0,:]\n",
    "            print(y.shape)#64\n",
    "            print(y_hat.shape)#64X1\n",
    "            print(y.type())#expect float got long???\n",
    "            print(y_hat.type())#float\n",
    "            y=torch.unsqueeze(y,1)#target 二维的 64X1\n",
    "            print(y.shape)\n",
    "            l=loss(y_hat,y).sum()\n",
    "            if optimizer is not None:\n",
    "                optimizer.zero_grad()\n",
    "            elif params is not None and params[0].grad is not None:\n",
    "                for param in params:\n",
    "                    param.grad.data.zero_()\n",
    "            l.backward()\n",
    "            if optimizer is None:\n",
    "                SGD(params,lr)\n",
    "            else:\n",
    "                optimizer.step()\n",
    "            train_l_sum+=l.item()\n",
    "            train_acc_sum+=(y_hat.argmax(dim=1)==y).sum().item()\n",
    "            n+=y.shape[0]\n",
    "        test_acc,test_l = evaluate_accuracy(test_iter,net,loss)\n",
    "        train_loss.append(train_l_sum/n)\n",
    "        test_loss.append(test_l)\n",
    "        print('epoch%d,loss%.4f,train acc %3f,test acc %.3f'%(epoch+1,train_l_sum/n,train_acc_sum/n,test_acc))\n",
    "    return train_loss,test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-185-f3d21157cbb3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_rnn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmy_rnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/perception/deep-learning/utils.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, train_iter, val_iter, test_iter, loss_fn, denormalize_fn, optimizer, num_epoch, early_stop, device, output_model, is_print, is_print_batch)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mbest_val_rmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m9999\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0mearly_stop_flag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput_model\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'float' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "train_loss,test_loss=train(my_rnn,train_loader,test_loader,loss_func,epochs,batch_size,my_rnn.parameters(),lr,optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch.utils import data\n",
    "import torch.nn as nn\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error as mse_fn, mean_absolute_error as mae_fn\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "def mape_fn(y, pred):\n",
    "    mask = y != 0\n",
    "    y = y[mask]\n",
    "    pred = pred[mask]\n",
    "    mape = np.abs((y - pred) / y)\n",
    "    mape = np.mean(mape) * 100\n",
    "    return mape\n",
    "\n",
    "\n",
    "def eval(y, pred):\n",
    "    y = y.cpu().numpy()\n",
    "    pred = pred.cpu().numpy()\n",
    "    mse = mse_fn(y, pred)\n",
    "    rmse = math.sqrt(mse)\n",
    "    mae = mae_fn(y, pred)\n",
    "    mape = mape_fn(y, pred)\n",
    "    return [rmse, mae, mape]\n",
    "\n",
    "\n",
    "# 测试函数（用于分类）\n",
    "def test(net, output_model, data_iter, loss_fn, denormalize_fn, device='cpu'):\n",
    "    rmse, mae, mape = 0, 0, 0\n",
    "    batch_count = 0\n",
    "    total_loss = 0.0\n",
    "    net.eval()\n",
    "    if output_model is not None:\n",
    "        output_model.eval()\n",
    "    for X, Y in data_iter:\n",
    "        X = X.to(device).float()\n",
    "        Y = Y.to(device).float()\n",
    "        output, hidden = net(X)\n",
    "        if output_model is not None:\n",
    "            y_hat = output_model(output[:, -1, :].squeeze(-1)).squeeze(-1)\n",
    "        else:\n",
    "            y_hat = output[:, -1, :].squeeze(-1)\n",
    "        loss = loss_fn(y_hat, Y)\n",
    "\n",
    "        Y = denormalize_fn(Y)\n",
    "        y_hat = denormalize_fn(y_hat)\n",
    "        a, b, c = eval(Y.detach(), y_hat.detach())\n",
    "        rmse += a\n",
    "        mae += b\n",
    "        mape += c\n",
    "        total_loss += loss.detach().cpu().numpy().tolist()\n",
    "        batch_count += 1\n",
    "    return [rmse / batch_count, mae / batch_count, mape / batch_count], total_loss / batch_count\n",
    "\n",
    "\n",
    "def train(net, train_iter, val_iter, test_iter, loss_fn, denormalize_fn, optimizer, num_epoch,\n",
    "          early_stop=10, device='cpu', output_model=None, is_print=True, is_print_batch=False):\n",
    "    train_loss_lst = []\n",
    "    val_loss_lst = []\n",
    "    train_score_lst = []\n",
    "    val_score_lst = []\n",
    "    epoch_time = []\n",
    "\n",
    "    best_epoch = 0\n",
    "    best_val_rmse = 9999\n",
    "    early_stop_flag = 0\n",
    "    for epoch in range(num_epoch):\n",
    "        net.train()\n",
    "        if output_model is not None:\n",
    "            output_model.train()\n",
    "        epoch_loss = 0\n",
    "        batch_count = 0\n",
    "        batch_time = []\n",
    "        rmse, mae, mape = 0, 0, 0\n",
    "        for X, Y in train_iter:\n",
    "            batch_s = time.time()\n",
    "            X = X.to(device).float()\n",
    "            Y = Y.to(device).float()\n",
    "            optimizer.zero_grad()\n",
    "            output, hidden = net(X)\n",
    "            if output_model is not None:\n",
    "                y_hat = output_model(output[:, -1, :].squeeze(-1)).squeeze()\n",
    "            else:\n",
    "                y_hat = output[:, -1, :].squeeze(-1)\n",
    "            loss = loss_fn(y_hat, Y)#input,target\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            Y = denormalize_fn(Y)\n",
    "            y_hat = denormalize_fn(y_hat)\n",
    "            a, b, c = eval(Y.detach(), y_hat.detach())\n",
    "            rmse += a\n",
    "            mae += b\n",
    "            mape += c\n",
    "            epoch_loss += loss.detach().cpu().numpy().tolist()\n",
    "            batch_count += 1\n",
    "            # sample_num += X.shape[0]\n",
    "\n",
    "            batch_time.append(time.time() - batch_s)\n",
    "            if is_print and is_print_batch:\n",
    "                print('epoch-batch: %d-%d, train loss %.4f, time use %.3fs' %\n",
    "                      (epoch + 1, batch_count, epoch_loss, batch_time[-1]))\n",
    "\n",
    "        train_loss = epoch_loss / batch_count\n",
    "        train_loss_lst.append(train_loss)\n",
    "        train_score_lst.append([rmse/batch_count, mae/batch_count, mape/batch_count])\n",
    "\n",
    "        # 验证集\n",
    "        val_score, val_loss = test(net, output_model, val_iter, loss_fn, denormalize_fn, device)\n",
    "        val_score_lst.append(val_score)\n",
    "        val_loss_lst.append(val_loss)\n",
    "\n",
    "        epoch_time.append(np.array(batch_time).sum())\n",
    "\n",
    "        # 打印本轮训练结果\n",
    "        if is_print:\n",
    "            print('*** epoch%d, train loss %.4f, train rmse %.4f, val loss %.4f, val rmse %.6f, time use %.3fs' %\n",
    "                  (epoch + 1, train_loss, train_score_lst[-1][0], val_loss, val_score[0], epoch_time[-1]))\n",
    "\n",
    "        # 早停\n",
    "        if val_score[0] < best_val_rmse:\n",
    "            best_val_rmse = val_score[0]\n",
    "            best_epoch = epoch\n",
    "            early_stop_flag = 0\n",
    "        else:\n",
    "            early_stop_flag += 1\n",
    "            if early_stop_flag == early_stop:\n",
    "                print(f'\\nThe model has not been improved for {early_stop} rounds. Stop early!')\n",
    "                break\n",
    "\n",
    "    # 输出最终训练结果\n",
    "    print(f'\\n{\"*\" * 40}\\nFinal result:')\n",
    "    print(f'Get best validation rmse {np.array(val_score_lst)[:, 0].min() :.4f} '\n",
    "          f'at epoch {best_epoch}')\n",
    "    print(f'Total time {np.array(epoch_time).sum():.2f}s')\n",
    "    print()\n",
    "\n",
    "    # 计算测试集效果\n",
    "    test_score, test_loss = test(net, output_model, test_iter, loss_fn, denormalize_fn, device)\n",
    "    print('Test result:')\n",
    "    print(f'Test RMSE: {test_score[0]}    Test MAE: {test_score[1]}    Test MAPE: {test_score[2]}')\n",
    "    return train_loss_lst, val_loss_lst, train_score_lst, val_score_lst, epoch\n",
    "\n",
    "\n",
    "def visualize(num_epochs, train_data, test_data, x_label='epoch', y_label='loss'):\n",
    "    x = np.arange(0, num_epochs + 1).astype(dtype=np.int)\n",
    "    plt.plot(x, train_data, label=f\"train_{y_label}\", linewidth=1.5)\n",
    "    plt.plot(x, test_data, label=f\"val_{y_label}\", linewidth=1.5)\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_metric(score_log):\n",
    "    score_log = np.array(score_log)\n",
    "\n",
    "    plt.figure(figsize=(10, 6), dpi=300)\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(score_log[:, 0], c='#d28ad4')\n",
    "    plt.ylabel('RMSE')\n",
    "\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(score_log[:, 1], c='#e765eb')\n",
    "    plt.ylabel('MAE')\n",
    "\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(score_log[:, 2], c='#6b016d')\n",
    "    plt.ylabel('MAPE')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "\"nll_loss_forward_reduce_cuda_kernel_2d_index\" not implemented for 'Float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-186-3887669ed349>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrain_loss_lst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss_lst\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     train_score_lst, val_score_lst, stop_epoch = train(my_rnn, train_loader, val_loader, test_loader,\n\u001b[0m\u001b[1;32m      5\u001b[0m                                                        \u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFourSquareData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdenormalize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                                                        early_stop=20, device=device, output_model=None)\n",
      "\u001b[0;32m~/perception/deep-learning/utils.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, train_iter, val_iter, test_iter, loss_fn, denormalize_fn, optimizer, num_epoch, early_stop, device, output_model, is_print, is_print_batch)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0;31m#y_hat = output_model(output[:, -1, :]).squeeze()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                 \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m                 \u001b[0;31m#y_hat = output[:, -1, :]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0;31m#print(y_hat.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1150\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m   1151\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m                                label_smoothing=self.label_smoothing)\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   2844\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2845\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2846\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: \"nll_loss_forward_reduce_cuda_kernel_2d_index\" not implemented for 'Float'"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "\n",
    "train_loss_lst, val_loss_lst, \\\n",
    "    train_score_lst, val_score_lst, stop_epoch = train(my_rnn, train_loader, val_loader, test_loader,\n",
    "                                                       loss_func, FourSquareData.denormalize, optimizer, epochs,\n",
    "                                                       early_stop=20, device=device, output_model=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(stop_epoch, train_loss_lst, val_loss_lst, y_label='Loss')\n",
    "plot_metric(train_score_lst)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c68792f7cd82460065c5d613846083f799a914df57fed46700f5555136f46f97"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
